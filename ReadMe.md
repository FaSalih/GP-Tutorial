# Gaussian Process Modeling Tutorial

This repository contains a comprehensive tutorial on Gaussian Process (GP) modeling, demonstrating the impact of different normalization methods and kernel choices on model performance.

## Project Structure

```
├── 00_gen_dummy_data.py    # Generate synthetic dataset
├── 01_split_dataset.py     # Split data into train/test sets
├── 02_train_gps.py        # Train GP models with different configurations
├── 03_deploy_model.py     # Deploy the trained GP model
├── 04_visualize.py        # Visualize results and model performance
├── dummy_data.csv         # Generated synthetic dataset
├── data-splits/           # Directory for train/test splits
└── utils/                 # Utility functions
    ├── gp_tools.py       # GP model building and prediction tools
    └── normalize.py      # Data normalization utilities
```

## Features

- Generation of synthetic data (noisy exponentially decaying sine function)
- Implementation of various normalization methods:
  - Standardization
  - Min-Max Scaling
  - Log Standardization
  - Log+b Standardization
  - Square Root Transformation
- Multiple kernel options:
  - RBF (Radial Basis Function)
  - RQ (Rational Quadratic)
  - Matérn 3/2
  - Matérn 5/2
- K-fold cross-validation support
- Visualization tools for model performance analysis (*not completed yet*)

## Getting Started

The scripts are named in the order they should be run, and help messages detailing the required arguments for each script can be generated by running `python <script.py> --help`. But example usage commands are shown below for one specific dummy example.

1. Ensure all dependencies are present by creating a conda environment from the provided environment file, and activating it before running any of the numbered scripts. From the main directory of the repo:
      ```bash
      conda env create -f env.yml
      conda activate gp-tutorial
      ```

1. Generate the synthetic dataset. If using your own data, ensure the format of your csv matches the format of the generated dummy data, i.e.
      - Index column at position 0.
      - All columns other than the target colum `Y` are features (csv includes no metadata).
      ```bash
      python 00_gen_dummy_data.py
      ```

1. Split the dataset into training and testing sets:
      ```bash
      python 01_split_dataset.py --csv dummy_data.csv --target Y --folds 3 --dir "data-splits" --folds 4 --bins 15
      ```

1. Train GP models with a pre-specified set of  normalization methods and kernels and a specific data-split of k-fold.
      ```bash
      python 02_train_gps.py --target Y --kfold 1 --dir "data-splits" 
      ```
      Or loop over all folds using:
      ```bash
      for k in {1..4}; do ( python 02_train_gps.py --target Y --kfold $k) done
      ```

1. Deploy the trained models. Loops over all combinations of normalization methods, kernels and folds and generates an output file for each combination in the format `predictions/<kernel>_<normalization>_fold_<k-fold number>.csv` (e.g. `predictions/RQ_MinMax_fold_1.csv`).
      ```bash
      python 03_deploy_model.py --csv dummy_data.csv --folds 4 --target Y
      ```
## Dependencies

- NumPy
- Pandas
- GPflow
- Scikit-learn
- Matplotlib
- tqdm

To ensure reproducibility, please install the conda environment as such: 

```
conda env create -f env.yml
```

## Usage Notes

- The synthetic dataset is generated using an exponentially decaying sine function with added Gaussian noise
- All scripts assume there is in index column at position 0 and that there are no meta-data columns (i.e. all columns other than the target column are features).
- Different normalization methods can significantly impact GP model performance
- The tutorial includes various kernel options to demonstrate their effects on modeling different types of data patterns
- Visualization tools help in understanding the model's performance and the effects of different configurations


